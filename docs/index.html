<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc-markdown-css-theme" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="" />
  <title>ModuLoRA: Finetuning 2-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers</title>
  <link rel="stylesheet" href="css/theme.css" />
  <link rel="stylesheet" href="css/skylighting-solarized-theme.css" />
  <script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" />
  <link rel="stylesheet" href="../public/css/tufte.css">
</head>
<body>

<header>
<h1 class="title">ModuLoRA: Finetuning 2-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers</h1>
<blockquote class="metadata">
</blockquote>
</header>

<nav id="TOC" role="doc-toc">
    <strong>Contents</strong><label for="contents">⊕</label>
  <input type="checkbox" id="contents">
  <ul>
  <li><a href="#introduction" id="toc-introduction">Introduction</a></li>
  <li><a href="#resources" id="toc-resources">Resources</a></li>
  <li><a href="#method-overview" id="toc-method-overview">Method Overview</a>
  <ul>
  <li><a href="#quantization-x-lora" id="toc-quantization-x-lora">Quantization x LoRA</a></li>
  <li><a href="#modularity" id="toc-modularity">Modularity</a></li>
  <li><a href="#llmtools" id="toc-llmtools">LLMTools</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results">Results</a>
  <ul>
  <li><a href="#natural-language-inference" id="toc-natural-language-inference">Natural Language Inference</a></li>
  <li><a href="#memory-usage" id="toc-memory-usage">Memory Usage</a></li>
  </ul></li>
  <li><a href="#usage" id="toc-usage">Usage</a>
  <ul>
  <li><a href="#how-to-use-llmtools" id="toc-how-to-use-llmtools">How to Use LLMTools</a></li>
  <li><a href="#advanced-usage" id="toc-advanced-usage">Advanced Usage</a>
  <ul>
  <li><a href="#enabling-npp-training" id="toc-enabling-npp-training">Enabling NPP Training</a></li>
  <li><a href="#enabling-ddp-training" id="toc-enabling-ddp-training">Enabling DDP Training</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#references" id="toc-references">References</a></li>
  </ul>
</nav>

<main>
<blockquote>
<p><a href="https://oseyincs.io/">Junjie Oscar Yinn</a>, <a href="https://www.linkedin.com/in/jiahaodong">Jiahao Dong</a>, <a href="https://isjakewong.github.io/">Yingheng Wang</a>, <a href="https://www.cs.cornell.edu/~cdesa/">Chris De Sa</a>, and <a href="https://www.cs.cornell.edu/~kuleshov/">Volodymyr Kuleshov</a>.<br />
Date: March 2024</p>
</blockquote>
<div class="wide extra-wide left-align-caption">
<p><img src="../docs/img/ModuLoRA-Figure.png" /></p>
</div>
<h1 id="introduction">Introduction</h1>
<p>Finetuning LLMs on consumer GPUs poses significant challenges due to LLMs’ sheer size and memory requirement. Finetuning LLMs have shown to be an essential task for developing interactive agents with instruction following finetuning <a href="https://arxiv.org/pdf/2212.10560">(Wang et al., 2022)</a> and powerful AI systems through RLHF <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf">(Ouyang et al., 2022)</a>. Thus, improving the memory-efficiency of LLM finetuning is an important step toward accessbile and practical LLMs application.</p>
<p>One promising class of methods for memory-efficient finetuning is parameter efficient finetuning (PEFT), which usually invovles learning a small adapter that can be applied to the pre-trained model <a href="https://www.nature.com/articles/s42256-023-00626-4">(Ding et al., 2023)</a>. PEFT reduces the memory requirement for finetuning LLMs by freezing its pre-trained parameters and optimizing a set of new parameters that is a fraction of the pre-trained parameters.</p>
<p>In this blogpost, we introduce <strong>ModuLoRA</strong>, a modular PEFT framework that integreates user-specified weight quantizer with finetuning via low-rank adapters (LoRAs). ModuLoRA allows 2-bit models to acheive near lossless downstream finetuning peformance compared to higher precision 4-bit, 8-bit, and even 16-bit models. Leveraging the transformers and peft libraries within the Hugging Face ecosystem, users can finetune and deploy 2/3/4-bit models easily. The abstract of the paper is as follows:</p>
<blockquote>
<p>We propose a memory-efficient finetuning algorithm for large language models (LLMs) that supports finetuning LLMs with 65B parameters in 2/3/4-bit precision on as little as one 24GB GPU. Our method, modular low-rank adaptation (ModuLoRA), integrates any user-specified weight quantizer with finetuning via low-rank adapters (LoRAs). Our approach relies on a simple quantization-agnostic backward pass that adaptively materializes lowprecision LLM weights from a custom black-box quantization module. This approach enables finetuning 2-bit and 3-bit LLMs for the first time—leveraging state-of-the-art 2-bit QuIP# quantization and 3-bit OPTQ quantization—outperforming finetuning that relies on less sophisticated 4-bit and 8-bit methods. In our experiments, ModuLoRA attains competitive performance on text classification, natural language inference, and instruction following tasks using significantly less memory than existing approaches, and we also surpass the state-of-the-art ROUGE score on a popular summarization task. We release ModuLoRA together with a series of low-precision models as part of LLMTools, a user-friendly library for quantizing, running, and finetuning LLMs on consumer GPUs.</p>
</blockquote>
<h1 id="resources">Resources</h1>
<p>We provide several resources with this blogpost to get started with ModuLoRA:</p>
<p><a href="https://arxiv.org/pdf/2309.16119">→ ModuLoRA Paper (TMLR with Featured Certificate)</a></p>
<p><a href="https://github.com/kuleshov-group/llmtools/">→ Codebase (LLMTools) for Easy Usage</a></p>
<h1 id="method-overview">Method Overview</h1>
<p>ModuLoRA is a research project at Cornell University, and is based on the following publications.</p>
<blockquote>
<ul>
<li>Junjie Yin, Jiahao Dong, Yingheng Wang, Christopher De Sa, Volodymyr Kuleshov ModuLoRA: Finetuning 2-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers. TMLR 2023 <strong>Featured Certificte</strong></li>
<li>Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, Christopher De Sa. QuIP: 2-Bit Quantization of Large Language Models with Guarantees. NeurIPS 2023 <strong>Spotlight</strong></li>
<li>Tseng, Albert, Jerry Chee, Qiyao Sun, Volodymyr Kuleshov, Christopher De Sa. “Quip#: Even better LLM quantization with hadamard incoherence and lattice codebooks.” arXiv preprint arXiv:2402.04396 (2024).</li>
</ul>
</blockquote>
<h2 id="quantization-x-lora">Quantization x LoRA</h2>
<p>ModuLoRA relies on two components: quantization and low rank adapation (LoRA) of LLMs. Quantization methods reduce the number of bits required to store model weights. Recently, OPTQ demonstrated the feasibility of quantizing to billion-parameter LLMs. A <span class="math inline">x</span>-bit quantization method has the form <span class="math display">
(\hat{W}_q, \mathbf{z}, \mathbf{s}) = \mathcal{Q}(\mathbf{W}) \quad \quad \hat{W} = \mathcal{D}(\hat{W}_q, \mathbf{z}, \mathbf{s}).
</span></p>
<p>Here, the quantization algorithm <span class="math inline">\mathcal{Q}</span> takes a weight matrix <span class="math inline">\mathbf{W} \in \mathbb{R}^{d \times d}</span> (or its subset) and outputs a quantized version <span class="math inline">\hat{W}_q \in \{0, 1, \ldots, 2^{b-1}\}^{d \times d}</span>, using <span class="math inline">x</span> bits to represent each entry of <span class="math inline">\mathbf{W}</span>, as well as zero and scale parameters <span class="math inline">\mathbf{z}, \mathbf{s} \in \mathbb{R}^d</span> (in full precision). The dequantization algorithm <span class="math inline">\mathcal{D}(\hat{W}_q, \mathbf{z}, \mathbf{s})</span> recovers an approximation <span class="math inline">\hat{W} \in \mathbb{R}^{d \times d}</span> by rescaling the quantized weights as <span class="math inline">\hat{W} = \mathbf{s} \odot \hat{W}_q + \mathbf{z}</span>, where <span class="math inline">\odot</span> denotes the Hadamard product. The operations <span class="math inline">\odot</span> and <span class="math inline">+</span>​ are defined to be compatible with numpy-style broadcasting.</p>
<p>Currently, ModuLoRA incorporates two quantization modules–OPTQ and QuIP#–that enables low-precision finetuning with little loss degradation. Recently, Frantar et al. (2023) proposed OPTQ, a quantization algorithm that scales to modern LLMs. The method iteatively runs two steps over the weight columns: (1) quantize with nearest rounding and compute the error, (2) update the remaining weights with a scaled error.</p>
<p>Following OPTQ, Chee et al. (2023) proposed QuIP, a quantization algorithm that makes two-bit LLM compression viable for the first time. The method follows improves upon OPTQ with an efficient pre- and post-processing procedure ensuring weight and Hessian incoherence through multiplication by random orthogonal matrices. Further, Tseng et al. (2023) proposed QuIP#, combining lattice codebooks with incoherence processing from QuIP to create state-of-the-art 2 bit quantized models. Both QuIP and QuIP# optimize weights through a per-layer “adaptive rounding” loss function, defined as: <span class="math display">
\ell(\hat W_q)
  = E_x \left[ \| (\hat W_q - \mathbf{W})x \|^2 \right]
  = \operatorname{tr}\left(
    (\hat W_q - \mathbf{W}) H (\hat W_q - \mathbf{W})^T
   \right).
</span> where <span class="math inline">x \in \mathbb{R}^n</span> is the input vector drawn from a calibration set uniformly and <span class="math inline">H</span>​ acts as the second-moment matrix for these vectors, analogous to a Hessian proxy.</p>
<p>For more details about the incorporated quantization module, please refer to the original <a href="https://arxiv.org/html/2402.04396v1">Quip#</a> and <a href="https://openreview.net/pdf?id=tcbBPnfwxS">OPTQ</a> paper.</p>
<p>The LoRA algorithm by <a href="https://arxiv.org/pdf/2106.09685">Hu et al. (2021)</a> decomposes the weights <span class="math inline">W</span> into a sum of frozen base model weights <span class="math inline">W_0 \in \mathbb{R}^{d \times d}</span> and a small additive low-rank adapter <span class="math inline">AB^T</span> consisting of the product of two rectangular matrices <span class="math inline">A, B \in \mathbb{R}^{d \times r}</span>, where <span class="math inline">r &gt; 0</span> indicates the rank<span class="sidenote-wrapper"><label for="sn-0" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-0" class="margin-toggle"/><span class="sidenote">For simplicity here we consider square weight matrices <span class="math inline">W</span>; the rectangular case is a straightforward generalization.<br />
<br />
</span></span>:</p>
<p><span class="math display">
W = W_0 + AB^T
</span></p>
<p>where matrix <span class="math inline">A</span> is being initialized to Gaussian noise and <span class="math inline">B</span> to 0 (s.t. <span class="math inline">AB=0</span> during the start of the training). This way, LoRA reparameterizes the foward pass of the linear layer as <span class="math display"> X(W) = X(W_0 + AB^T) </span>, where <span class="math inline">X</span> is the previous layer’s activation, and only <span class="math inline">A</span> and <span class="math inline">B</span> received weights updates during finetuning. Unlike full finetuning methods, LoRA is very memory efficient in model finetuning, as it doesn’t require extra GPU memory for gradient and optimizer state storage.</p>
<h2 id="modularity">Modularity</h2>
<p>We released the first version of our method in April 2023 <a href="https://github.com/kuleshov-group/llmtools/tree/dev">link</a>, and have since been refining it based on user feedback. In a parallel effort, Dettmers et al. (2023) proposed QLoRA, an approach for tuning quantized 4-bit LLMs based on LoRA. The method uses a novel quantization data type 4-bit NormalFloat to achieve competitive results with full finetuning. Subsequent work such as LP-LoRA follows QLoRA’s quantization scheme and proposes a dynamic method to configure quantization parameters <a href="https://arxiv.org/html/2311.12023v2">(Guo et al., 2023)</a>. However, these approaches predefine a quantization scheme that makes them challenging to scale as more advanced quantization scheme are being developed and used.</p>
<p>In our work, we explore the use of modular quantizer by decomposing quantization finetuning tasks into a more general form that may be handled by our quantization-agnostic forward/backward pass. Our quantization-agnostic module compose an simple but efficient backward pass for different quantization scheme. Crucially, ModuLoRA does not specificy a particular quantization scheme; rather, it incorporates a quantization-agnostic backward pass that enables flexible finetuning in desired precision. These advantages make ModuLoRA an easy-to-use, performant, and modular PEFT framework.</p>
<h2 id="llmtools">LLMTools</h2>
<p>We implement ModuLoRA as part of LLMTools, a compact, user friendly library that enables users to finetune and perform inference on the largest LLMs on consumer hardware. The LLMTools library enables finetuning LLMs in 2-bit, 3-bit, and 4-bit precision using the ModuLoRA algorithm. It also provides an easy-to-use Python API for quantization, inference, and finetuning, as well as modular support for multiple quantizers, LLMs (including LLaMA1, LLaMA2, BLOOM, and OPT), and optimization algorithms (including all that are compatible with the Hugging Face Trainer class). Lastly, LLMTools supports easily loading datasets and sharing models via the HuggingFace Hub.</p>
<p>Please see <a href="#usage">usage</a> for example usage of LLMTools.</p>
<h1 id="results">Results</h1>
<p>Here, we present some ModuLoRA results on a popular natural langauge inference benchmark. More experiment results can be found in the <a href="https://arxiv.org/pdf/2309.16119">paper</a>.</p>
<h2 id="natural-language-inference">Natural Language Inference</h2>
<p>We finetune 7B to 65B LLaMA models on the Multi-Genre Natural Language Inference Corpus (MNLI) <a href="https://arxiv.org/pdf/1704.05426">(Williams et al., 2018)</a> and evaluate on the matched test sets (in-domain examples), reporting accuracy.</p>
<p><em>Models.</em> We evaluate ModuLoRA and on the LLaMA-1 family of models. We quantize the models to 3 bits and 4 bits using OPTQ as in [4] with calibration 128 samples from C4 [5]. We quantize the models to 2 bits using QuIP# as in [6, 7] with <span class="math inline">E_8</span> lattice codebooks.</p>
<p><em>Baseline.</em> We use LoRA (as implemented in the PEFT library <a href="https://huggingface.co/blog/peft">(Mangrulkar et al., 2022)</a>) to finetune models quantized in 8 bits using the BitsAndBytes library <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/c3ba4962c05c49636d4c6206a97e9c8a-Abstract-Conference.html">(Dettmers et al., 2022)</a>; we also compare to full-precision results from the literature. In recent work, <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/1feb87871436031bdc0f2beaa62a049b-Paper-Conference.pdf">Dettmers et al. (2023)</a> proposed QLoRA, a related 4-bit finetuning algorithm implemented in the BitsAndBytes library.</p>
<p><em>Training.</em> We finetune all models on NVIDIA TITAN, 3090, and A6000 GPUs (depending on the model) with a LoRA rank of r = 8 and alpha of a = 32, and report results from 3 random seeds. We set up the training procedure following LoRA paper, with slight variation to accommodate our particular language models.</p>
<p>Our 2-bit and 3-bit 65B LLaMA model matches the performance of a full-precision GPT-3+LoRA baseline. <strong>Notably, 2-bit 65B models finetuned with QuIP# outperforms the rest of 65B models with higher precisions.</strong> We also find that 4-bit models from LLMTools outperform 8-bit models from the Bits&amp;Bytes library for the entire model size range. 2-bit, 3-bit and 4-bit ModuLoRA models either match or outperform their 4-bit QLoRA counterparts, often using less memory because of lower precision models.</p>
<table>
<colgroup>
<col style="width: 18%" />
<col style="width: 24%" />
<col style="width: 11%" />
<col style="width: 25%" />
<col style="width: 19%" />
</colgroup>
<thead>
<tr class="header">
<th>MNLI-m Baselines</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Models</strong></td>
<td>Finetuning Adaptation</td>
<td>Model Size</td>
<td># Trainable Parameters</td>
<td>MNLI-m (accuracy)</td>
</tr>
<tr class="even">
<td>GPT-3</td>
<td>Full Finetuning</td>
<td>175B</td>
<td>175,255.8M</td>
<td>89.5 ± 0.1</td>
</tr>
<tr class="odd">
<td>GPT-3</td>
<td>Adapter</td>
<td>175B</td>
<td>40.1M</td>
<td>91.5 ± 0.1</td>
</tr>
<tr class="even">
<td>GPT-3</td>
<td>LoRA</td>
<td>175B</td>
<td>4.7M</td>
<td>91.7 ± 0.1</td>
</tr>
<tr class="odd">
<td>T5</td>
<td>Full Finetuning</td>
<td>11B</td>
<td>11,307.4M</td>
<td><strong>92.2 ± 0.1</strong></td>
</tr>
</tbody>
</table>
<table>
<caption>Natural language inference on the MNLI-m dataset evaluated using classification accuracy (%). Our LLaMA-65B-2bit model approaches state-of-the-art scores using significantly less memory.</caption>
<colgroup>
<col style="width: 24%" />
<col style="width: 17%" />
<col style="width: 13%" />
<col style="width: 13%" />
<col style="width: 13%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr class="header">
<th>MNLI-m Results</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>LLaMA Finetuning</strong></td>
<td><strong>Quantizer</strong></td>
<td><strong>7B</strong></td>
<td><strong>13B</strong></td>
<td><strong>30B</strong></td>
<td><strong>65B</strong></td>
</tr>
<tr class="even">
<td>LLMTools (2-bit)</td>
<td>QuIP#(<span class="math inline">E_8</span>)</td>
<td>88.50 ± 0.3</td>
<td>89.72 ± 0.3</td>
<td>91.30 ± 0.3</td>
<td><strong>91.85 ± 0.3</strong></td>
</tr>
<tr class="odd">
<td>LLMTools (3-bit)</td>
<td>OPTQ</td>
<td>88.98 ± 0.2</td>
<td>90.20 ± 0.2</td>
<td>91.09 ± 0.2</td>
<td>91.42 ± 0.1</td>
</tr>
<tr class="even">
<td>LLMTools (4-bit)</td>
<td>OPTQ</td>
<td>89.31 ± 0.2</td>
<td>90.41 ± 0.2</td>
<td>91.31 ± 0.1</td>
<td>91.59 ± 0.2</td>
</tr>
<tr class="odd">
<td>Bits&amp;Bytes (4-bit)</td>
<td>QLoRA</td>
<td>89.28 ± 0.2</td>
<td>89.67 ± 0.2</td>
<td>91.22 ± 0.1</td>
<td>91.36 ± 0.2</td>
</tr>
<tr class="even">
<td>Bits&amp;Bytes (8-bit)</td>
<td>LLM.int8()</td>
<td>88.95 ± 0.1</td>
<td>90.08 ± 0.1</td>
<td>91.15 ± 0.1</td>
<td>91.55 ± 0.1</td>
</tr>
</tbody>
</table>
<p><label for="mn-demo" class="margin-toggle">⊕</label> <input type="checkbox" id="mn-demo" class="margin-toggle"/> <span class="marginnote"> MNLI-m acuracy gap to 8-bit BitsBytes. Notably, 2-bit LLMTools outperforms 8-bit BitBytes for larger models, 30B and 65B respectively. 4-bit models from LLMTools outperform 8-bit models from the BitsBytes library for the entire model size range. </span></p>
<figure>
<img src="../docs/img/Accuracy-Gap-MNLI-M.png" alt="Accuracy Gap MNLI-M" />
<figcaption aria-hidden="true">Accuracy Gap MNLI-M</figcaption>
</figure>
<!-- 
<img src="/Users/osie/Desktop/Accuracy Gap MNLI-M.png" alt="Accuracy Gap MNLI-M" style="zoom:50%;" /> -->
<h2 id="memory-usage">Memory Usage</h2>
<p>We show the memory required to perform finetuning on MNLI-M for different LLaMA model sizes in table down below. ModuLoRA significantly minimizes the memory requirements for finetuning on these models. We plot the memory requirements in figure as shown below for better visualization. As the model size increases to 65B, ModuLoRA uses only about 6% of the memory to run memory-efficient finetuning method LoRA. As the table and figure illustrates, with ModuLoRA it’s possible to not only run inference but also finetune 65B model on a single 24GB GPU. To produce this table, we run our quantizer-agnostic forward/backward passes for the entire LLaMA model size range with batch size 1 and maximum sequence length 128 on MNLI-m.</p>
<table>
<caption>Memory requirements to finetune LLaMA models on MNLI-M with batch size 1 and maximum sequence length 128.For comparison we include the memory requirements to finetune on LoRA and QLoRA.</caption>
<thead>
<tr class="header">
<th>Memory Usage on MNLI-m</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>LLaMA Finetuning</strong></td>
<td><strong>7B</strong></td>
<td><strong>13B</strong></td>
<td><strong>30B</strong></td>
<td><strong>65B</strong></td>
</tr>
<tr class="even">
<td>LLMTools (2-bit)</td>
<td>3.2 GB</td>
<td>5.4 GB</td>
<td>11.4 GB</td>
<td>21.8 GB</td>
</tr>
<tr class="odd">
<td>QLoRA (4-bit)</td>
<td>5.2 GB</td>
<td>8.6 GB</td>
<td>19.5 GB</td>
<td>36.7 GB</td>
</tr>
<tr class="even">
<td>Full Precision (LoRA)</td>
<td>38.4 GB</td>
<td>73.9 GB</td>
<td>183.3 GB</td>
<td>360.4 GB</td>
</tr>
</tbody>
</table>
<figure>
<img src="../docs/img/Memory-Requirement-MNLI-M.png" style="width: 70%; margin: auto;" alt="Visualization of Memory Requirements for MNLI-M" />
<figcaption aria-hidden="true">Visualization of Memory Requirements for MNLI-M</figcaption>
</figure>
<h1 id="usage">Usage</h1>
<p>In this section we introduce the transformers and peft integration of ModuLoRA and show how to use it with the latest QuIP# quantization module. LLMTools comes with a patched version of the PEFT library that can be used to finetune the quantized models using the ModuLoRA method.</p>
<p><label for="mn-demo" class="margin-toggle">⊕</label> <input type="checkbox" id="mn-demo" class="margin-toggle"/> <span class="marginnote"> The QuIP# quantization module supports 4-bit and 2-bit E8 codebooks finetuning. A full suite of 2 bit Llama 1 and 2 models quantized using QuIP# can be found here. <a href="https://huggingface.co/relaxml">Model Zoo</a> </span></p>
<p>As a quickstart, we provide a detailed installation for our ModuLoRA library <a href="https://github.com/kuleshov-group/llmtools">LLMTools</a>.</p>
<h2 id="how-to-use-llmtools">How to Use LLMTools</h2>
<p>To load a model that is being quantized in QuIP# is simply to pass the argument <code>load_in_quip=True</code> when calling the <code>from_pretrained</code> method by providing a device map (device map are defaulted to “auto” for automatic inference on devices).</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python numberLines"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">from</span> llmtools.llms.autollm <span class="im">import</span> AutoLLMForCausalLM</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a>model_name <span class="op">=</span> <span class="st">&quot;relaxml/Llama-1-65b-E8P-2Bit&quot;</span></span>
<span id="cb1-5"><a href="#cb1-5"></a></span>
<span id="cb1-6"><a href="#cb1-6"></a>llm, config <span class="op">=</span> AutoModelForCausalLM.from_pretrained(model_name, load_in_quip<span class="op">=</span><span class="va">True</span>, device_map<span class="op">=</span><span class="st">&quot;auto&quot;</span>)</span>
<span id="cb1-7"><a href="#cb1-7"></a>llm.<span class="bu">eval</span>()</span>
<span id="cb1-8"><a href="#cb1-8"></a></span>
<span id="cb1-9"><a href="#cb1-9"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name, device_map<span class="op">=</span>”auto“, use_fast<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-10"><a href="#cb1-10"></a>tokenizer.pad_token <span class="op">=</span> tokenizer.eos_token</span>
<span id="cb1-11"><a href="#cb1-11"></a>...</span></code></pre></div>
<p>The quantized QuIP# models are avaliable on HF Hub. See the complete model zoo <a href="https://github.com/Cornell-RelaxML/quip-sharp">here</a>. To use them, pass the given HF repo_id to <code>--hf_path</code>. Currently, LLMTools supports 2 and 4 bit models inference/finetuning.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python numberLines"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">from</span> llmtools.engine.lora.config <span class="im">import</span> FinetuneConfig</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">from</span> llmtools.engine.lora.peft <span class="im">import</span> quant_peft</span>
<span id="cb2-3"><a href="#cb2-3"></a></span>
<span id="cb2-4"><a href="#cb2-4"></a>tune_config <span class="op">=</span> FinetuneConfig(</span>
<span id="cb2-5"><a href="#cb2-5"></a>    <span class="co"># ... set up finetuning config</span></span>
<span id="cb2-6"><a href="#cb2-6"></a>)</span>
<span id="cb2-7"><a href="#cb2-7"></a></span>
<span id="cb2-8"><a href="#cb2-8"></a></span>
<span id="cb2-9"><a href="#cb2-9"></a>lora_config <span class="op">=</span> quant_peft.LoraConfig(</span>
<span id="cb2-10"><a href="#cb2-10"></a>    <span class="co"># ... create a lora config object</span></span>
<span id="cb2-11"><a href="#cb2-11"></a>)</span>
<span id="cb2-12"><a href="#cb2-12"></a>model <span class="op">=</span> quant_peft.get_peft_model(llm, lora_config)</span></code></pre></div>
<p>You can configure your hyperparameters for finetuning in the Finetuning config. Similarly, you can control how LoRA is applied to the base model through the Lora config. For more information about setting up LoRA, visit the official HF <a href="https://huggingface.co/docs/peft/main/en/conceptual_guides/lora">LoRA documentation</a>.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python numberLines"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">from</span> transformers <span class="im">import</span> TrainingArguments</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="im">from</span> llmtools.engine.hf.trainer <span class="im">import</span> Trainer</span>
<span id="cb3-3"><a href="#cb3-3"></a></span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="co"># load your desired data</span></span>
<span id="cb3-5"><a href="#cb3-5"></a>data <span class="op">=</span> <span class="co"># ... load the data</span></span>
<span id="cb3-6"><a href="#cb3-6"></a></span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="co"># training args</span></span>
<span id="cb3-8"><a href="#cb3-8"></a>training_arguments <span class="op">=</span> TrainingArguments(</span>
<span id="cb3-9"><a href="#cb3-9"></a>    <span class="co"># ... set up batch size, etc., in the usual way</span></span>
<span id="cb3-10"><a href="#cb3-10"></a>)</span>
<span id="cb3-11"><a href="#cb3-11"></a></span>
<span id="cb3-12"><a href="#cb3-12"></a><span class="co"># start trainer</span></span>
<span id="cb3-13"><a href="#cb3-13"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb3-14"><a href="#cb3-14"></a>    model<span class="op">=</span>model,</span>
<span id="cb3-15"><a href="#cb3-15"></a>    train_dataset<span class="op">=</span>data.train_data,</span>
<span id="cb3-16"><a href="#cb3-16"></a>    eval_dataset<span class="op">=</span>data.val_data,</span>
<span id="cb3-17"><a href="#cb3-17"></a>    args<span class="op">=</span>training_arguments,</span>
<span id="cb3-18"><a href="#cb3-18"></a>    data_collator<span class="op">=</span>transformers.DataCollatorForLanguageModeling(</span>
<span id="cb3-19"><a href="#cb3-19"></a>        tokenizer, mlm<span class="op">=</span><span class="va">False</span></span>
<span id="cb3-20"><a href="#cb3-20"></a>    ),</span>
<span id="cb3-21"><a href="#cb3-21"></a>)</span>
<span id="cb3-22"><a href="#cb3-22"></a></span>
<span id="cb3-23"><a href="#cb3-23"></a><span class="co"># start training</span></span>
<span id="cb3-24"><a href="#cb3-24"></a>trainer.train()</span>
<span id="cb3-25"><a href="#cb3-25"></a></span>
<span id="cb3-26"><a href="#cb3-26"></a></span>
<span id="cb3-27"><a href="#cb3-27"></a><span class="co"># Save Model</span></span>
<span id="cb3-28"><a href="#cb3-28"></a>model.save_pretrained(tune_config.lora_out_dir)</span></code></pre></div>
<p>Once you prepared your ModuLoRA model, you can use the HF Trainers and TrainingArguments to start your training.</p>
<p>For more examples of how to perform model quantization, inference, and finetuning take a look at the <code>examples</code> folder in the <a href="https://github.com/kuleshov-group/llmtools">LLMTools repo</a>.</p>
<h2 id="advanced-usage">Advanced Usage</h2>
<p>You can also finetune the quantized models with as many GPUs as you want. We provide two ways of parallelism to scale up your training.</p>
<h3 id="enabling-npp-training">Enabling NPP Training</h3>
<p><br/> The LLMTools library supports naive pipeline parallelism (NPP) training for our incorporated quantized models. NPP is a straightforward method for distributing a model across multiple GPUs. By loading both the model and its adapters onto several GPUs, NPP enables the basic communication of activations and gradients across GPUs. This approach essentially evenly fits the model across all available GPUs.</p>
<p><strong>How To Use NPP</strong></p>
<p>Check out this example on how to launch NPP training.</p>
<p>You need to set up your device map such that the process will dispatch model’s module correctly on multiple GPUs.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python numberLines"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>num_of_gpus <span class="op">=</span> torch.cuda.device_count()</span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="cf">if</span> num_of_gpus <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb4-3"><a href="#cb4-3"></a>    <span class="bu">print</span>(<span class="st">&quot;Enabling Naive Pipeline Parallel&quot;</span>)</span>
<span id="cb4-4"><a href="#cb4-4"></a>    max_memory <span class="op">=</span> get_balanced_memory(</span>
<span id="cb4-5"><a href="#cb4-5"></a>        model,</span>
<span id="cb4-6"><a href="#cb4-6"></a>        max_memory<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb4-7"><a href="#cb4-7"></a>        no_split_module_classes<span class="op">=</span>[<span class="st">&quot;LlamaDecoderLayer&quot;</span>, <span class="st">&quot;LlamaMLP&quot;</span>],</span>
<span id="cb4-8"><a href="#cb4-8"></a>        dtype<span class="op">=</span><span class="st">&#39;float16&#39;</span>,</span>
<span id="cb4-9"><a href="#cb4-9"></a>        low_zero<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb4-10"><a href="#cb4-10"></a>    )</span>
<span id="cb4-11"><a href="#cb4-11"></a></span>
<span id="cb4-12"><a href="#cb4-12"></a>    device_map <span class="op">=</span> infer_auto_device_map(</span>
<span id="cb4-13"><a href="#cb4-13"></a>        model,</span>
<span id="cb4-14"><a href="#cb4-14"></a>        max_memory<span class="op">=</span>max_memory,</span>
<span id="cb4-15"><a href="#cb4-15"></a>        no_split_module_classes<span class="op">=</span>[<span class="st">&quot;LlamaDecoderLayer&quot;</span>, <span class="st">&quot;LlamaMLP&quot;</span>],</span>
<span id="cb4-16"><a href="#cb4-16"></a>        dtype<span class="op">=</span><span class="st">&#39;float16&#39;</span></span>
<span id="cb4-17"><a href="#cb4-17"></a>    )</span>
<span id="cb4-18"><a href="#cb4-18"></a></span>
<span id="cb4-19"><a href="#cb4-19"></a>    model <span class="op">=</span> dispatch_model(model, device_map<span class="op">=</span>device_map)</span></code></pre></div>
<h3 id="enabling-ddp-training">Enabling DDP Training</h3>
<p><br/> The LLMTools library also supportsData Distributed Parallel (DDP) Training. DDP duplicates the model from GPU 0 to all other GPUs. For every batch, each GPU processes its own mini-batch of data independently. During the backward pass, after local gradients have been calculated, they are averaged across all participating processes, facilitating efficient parallel processing and synchronization among the GPUs.</p>
<p>Note that DDP should work <strong>if and only if</strong> the training setup (meaning model weights, gradients + intermediate hidden states) can entirely fit a single GPU.</p>
<p><strong>How To Use DDP</strong></p>
<p>Check out this example on how to launch DDP training.</p>
<p>You need to set up device_map such that each working process will load the entire model on the correct GPU. You can set up the device_map as followed:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python numberLines"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>device_index <span class="op">=</span> Accelerator().process_index</span>
<span id="cb5-2"><a href="#cb5-2"></a>device_map <span class="op">=</span> {<span class="st">&quot;&quot;</span>: device_index}</span></code></pre></div>
<p>If used, gradient accumulation step should be evently split on multiple GPUs:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python numberLines"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>world_size <span class="op">=</span> <span class="bu">int</span>(os.environ.get(<span class="st">&quot;WORLD_SIZE&quot;</span>, <span class="dv">1</span>))</span>
<span id="cb6-2"><a href="#cb6-2"></a>ddp <span class="op">=</span> world_size <span class="op">!=</span> <span class="dv">1</span></span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="cf">if</span> ddp:</span>
<span id="cb6-4"><a href="#cb6-4"></a>    num_of_gpus <span class="op">=</span> torch.cuda.device_count()</span>
<span id="cb6-5"><a href="#cb6-5"></a>    device_map <span class="op">=</span> {<span class="st">&quot;&quot;</span>: <span class="bu">int</span>(os.environ.get(<span class="st">&quot;LOCAL_RANK&quot;</span>) <span class="kw">or</span> <span class="dv">0</span>)}</span>
<span id="cb6-6"><a href="#cb6-6"></a>    gradient_accumulation_steps <span class="op">=</span> tune_config.batch_size <span class="op">//</span> (tune_config.mbatch_size<span class="op">*</span>num_of_gpus)</span>
<span id="cb6-7"><a href="#cb6-7"></a>    <span class="bu">print</span>(<span class="st">&quot;gradient_accumulation_steps: &quot;</span>, gradient_accumulation_steps)</span></code></pre></div>
<p>You can launch ModuLoRA Finetuning with DDP training by using:</p>
<pre><code>accelerate launch {script_name.py} --arg1 --arg2 ...</code></pre>
<p>For more information on how to use accelerate, please see the official <a href="https://huggingface.co/docs/accelerate/en/basic_tutorials/launch">accelerate doc</a> in HuggingFace.</p>
<a href="https://github.com/kuleshov-group/llmtools/" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>
<style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
<h1 id="references">References</h1>
<p><em>Chee, Jerry, et al. “Quip: 2-bit quantization of large language models with guarantees.” Advances in Neural Information Processing Systems 36 (2024).</em></p>
<p><em>Dettmers, Tim, et al. “Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale.” Advances in Neural Information Processing Systems 35 (2022): 30318-30332.</em></p>
<p><em>Ding, Ning, et al. “Parameter-efficient fine-tuning of large-scale pre-trained language models.” Nature Machine Intelligence 5.3 (2023): 220-235.</em></p>
<p><em>Frantar, Elias, et al. “OPTQ: Accurate quantization for generative pre-trained transformers.” The Eleventh International Conference on Learning Representations. 2022.</em></p>
<p><em>Hu, Edward J., et al. “Lora: Low-rank adaptation of large language models.” arXiv preprint arXiv:2106.09685 (2021).</em></p>
<p><em>Guo, Han, et al. “Lq-lora: Low-rank plus quantized matrix decomposition for efficient language model finetuning.” arXiv preprint arXiv:2311.12023 (2023).</em></p>
<p><em>Mangrulkar, Sourab, et al. “Peft: State-of-the-art parameter-efficient fine-tuning methods.” Younes Belkada and Sayak Paul,” PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods (2022).</em></p>
<p><em>Ouyang, Long, et al. “Training language models to follow instructions with human feedback.” Advances in neural information processing systems 35 (2022): 27730-27744.</em></p>
<p><em>Tseng, Albert, et al. “Quip#: Even better LLM quantization with hadamard incoherence and lattice codebooks.” arXiv preprint arXiv:2402.04396 (2024).</em></p>
<p><em>Wang, Yizhong, et al. “Self-instruct: Aligning language models with self-generated instructions.” arXiv preprint arXiv:2212.10560 (2022).</em></p>
<p><em>Williams, Adina, Nikita Nangia, and Samuel R. Bowman. “A broad-coverage challenge corpus for sentence understanding through inference.” arXiv preprint arXiv:1704.05426 (2017).</em></p>
<p class="signoff">
<a href="/">↑ Back to the top</a>
</p>
</main>

<script>
;(function() {
  // Non-essential if user has JavaScript off. Just makes checkboxes look nicer.
  var selector = '.task-list > li > input[type="checkbox"]';
  var checkboxes = document.querySelectorAll(selector);
  Array.from(checkboxes).forEach((checkbox) => {
    var wasChecked = checkbox.checked;
    checkbox.disabled = false;
    checkbox.addEventListener('click', (ev) => {ev.target.checked = wasChecked});
  });
})();
</script>
</body>
</html>
